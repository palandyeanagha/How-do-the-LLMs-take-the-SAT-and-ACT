{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11312441,"sourceType":"datasetVersion","datasetId":7075449}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install bert-score shap --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport json\nimport csv\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport shap\nimport os\nimport random\nimport glob\nfrom tqdm import tqdm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom bert_score import score as bert_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CONFIG\npath_to_nlu_dir = \"/kaggle/input/\"\ndata_dir = path_to_nlu_dir #+ \"nlu-data/\"\npath_to_res = \"/kaggle/working/\"\nsave_dir = path_to_res + \"results\"\nos.makedirs(save_dir, exist_ok=True)\n\ndatasets = [\n    #{\"path\": \"RACE-H_v1_tst.jsonl\", \"name\": \"RACE-H\"}\n    {\"path\": \"SATACT_v3_tst.jsonl\", \"name\": \"SATACT\"}\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = \"Salm00n/gpt2-xl_SATACT_v3\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name).to(device)\ntokenizer.pad_token = tokenizer.eos_token\nmodel.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"GLOBAL_Y_TRUE = []\nGLOBAL_Y_PRED = []\nGLOBAL_LOGITS = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data(filepath):\n    with open(filepath, 'r') as f:\n        lines = f.readlines()\n    return [json.loads(l) for l in lines]\n\ndef get_prompt(example):\n    if 'prompt' in example and example['prompt']:\n        return example['prompt']\n    elif 'context' in example and 'question' in example:\n        return example['context'] + \" \" + example['question']\n    elif 'question' in example:\n        return example['question']\n    else:\n        return \"\"\n\ndef get_options(example):\n    if \"options\" in example and example[\"options\"]:\n        return example[\"options\"]\n    elif all(k in example for k in [\"answerA\", \"answerB\", \"answerC\", \"answerD\"]):\n        return [example[\"answerA\"], example[\"answerB\"], example[\"answerC\"], example[\"answerD\"]]\n    else:\n        return []\n\ndef prepare_input(prompt):\n    enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=False).to(device)\n    if enc['input_ids'].shape[-1] > 1024:\n        enc['input_ids'] = enc['input_ids'][:, -1024:]\n        enc['attention_mask'] = enc['attention_mask'][:, -1024:]\n    return enc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(model, batch):\n    preds = []\n    logits_all_examples = []\n    option_labels = [\"A\", \"B\", \"C\", \"D\"]\n    for example in batch:\n        prompt = get_prompt(example)\n        options = get_options(example)\n        if not options:\n            continue\n        log_probs = []\n        for opt in options:\n            full_input = prompt + \" \" + opt\n            tokens = prepare_input(full_input)\n            input_ids = tokens[\"input_ids\"]\n            attention_mask = tokens[\"attention_mask\"]\n            with torch.no_grad():\n                output = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n            logits = output.logits\n            opt_tokens = tokenizer(opt, return_tensors=\"pt\")[\"input_ids\"][0].to(device)\n            seq_logits = logits[0, -opt_tokens.size(0)-1:-1, :]\n            log_prob = sum(torch.log_softmax(seq_logits[i], dim=-1)[opt_tokens[i]].item() for i in range(opt_tokens.size(0)))\n            log_probs.append(log_prob)\n        pred_idx = np.argmax(log_probs)\n        preds.append(pred_idx)\n        logits_all_examples.append(log_probs)\n    return preds, logits_all_examples","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_attention_map(model, input_text):\n    inputs = prepare_input(input_text)\n    with torch.no_grad():\n        outputs = model(**inputs, output_attentions=True)\n    attn = outputs.attentions[-1][0]\n    mean_attn = attn.mean(dim=0).cpu().numpy()\n    return mean_attn, tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n\ndef plot_attention_heatmap(attn_map, tokens, max_tokens=10):\n    attn_map = attn_map[:max_tokens, :max_tokens]\n    tokens = tokens[:max_tokens]\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(attn_map, xticklabels=tokens, yticklabels=tokens, cmap=\"coolwarm\")\n    plt.title(\"GPT-2 XL Final Layer Attention\", fontsize=18, fontweight='bold')\n    plt.xticks(rotation=90, fontsize=12)\n    plt.yticks(rotation=0, fontsize=12)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def true_attention_rollout(model, input_text):\n    inputs = prepare_input(input_text)\n    with torch.no_grad():\n        outputs = model(**inputs, output_attentions=True)\n    attentions = outputs.attentions\n    rollout = torch.stack(attentions).squeeze(1).mean(dim=1).mean(dim=0)\n    rollout_np = rollout.cpu().numpy()\n    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n    return rollout_np, tokens\n\ndef plot_attention_rollout(attn_rollout, tokens, max_tokens=10):\n    attn_rollout = attn_rollout[:max_tokens, :max_tokens]\n    tokens = tokens[:max_tokens]\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(attn_rollout, xticklabels=tokens, yticklabels=tokens, cmap=\"coolwarm\")\n    plt.title(\"True Attention Rollout Across All Layers (Mean Heads)\", fontsize=18, fontweight='bold')\n    plt.xticks(rotation=90, fontsize=12)\n    plt.yticks(rotation=0, fontsize=12)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def logit_lens_visual(model, input_text):\n    tokens = prepare_input(input_text)\n    with torch.no_grad():\n        outputs = model(**tokens, output_hidden_states=True)\n\n    logits = outputs.logits[0]  \n    confidences = logits.softmax(dim=-1).max(dim=-1).values.cpu().numpy()\n\n    return confidences","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def model_predict_for_shap(texts):\n    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits = outputs.logits[:, -1, :]  \n    return logits.cpu().numpy()\n\nmasker = shap.maskers.Text(tokenizer)\nexplainer = shap.Explainer(model_predict_for_shap, masker)\n\ndef fast_compute_shap_values(example):\n    prompt = get_prompt(example)\n\n    if not isinstance(prompt, str) or prompt.strip() == \"\":\n        return None\n\n    try:\n        input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids[0]\n\n        if len(input_ids) > 20:\n            selected_idxs = torch.randperm(len(input_ids))[:20]\n            input_ids = input_ids[selected_idxs.sort().values]\n\n        sampled_prompt = tokenizer.decode(input_ids, skip_special_tokens=True)\n\n        if not isinstance(sampled_prompt, str) or sampled_prompt.strip() == \"\" or len(sampled_prompt.split()) < 3:\n            return None\n\n        shap_values = explainer([sampled_prompt])\n        return float(np.mean(np.abs(shap_values.values)))\n\n    except:\n        return None  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def shallow_heuristic_check(question, options):\n    lower_q = question.lower()\n    matches = [int(opt.lower() in lower_q or any(word in lower_q for word in opt.lower().split())) for opt in options]\n    return matches\n\ndef context_fidelity_score(model, example):\n    options = get_options(example)\n    question = example.get(\"question\", \"\")\n    if 'context' in example:\n        prompt_full = example['context'] + \" \" + question\n    else:\n        prompt_full = question\n    prompt_no_context = question.strip()\n    if not options or not question:\n        return None\n    try:\n        pred_full = predict(model, [{\"prompt\": prompt_full, \"options\": options}])[0][0]\n        pred_no_context = predict(model, [{\"prompt\": prompt_no_context, \"options\": options}])[0][0]\n        return 1.0 if pred_full != pred_no_context else 0.0\n    except:\n        return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reasoning_score(model, input_text, options):\n    try:\n        tokens = tokenizer(input_text, return_tensors=\"pt\").to(device)\n        input_ids = tokens['input_ids']\n        baseline_pred, _ = predict(model, [{\"prompt\": input_text, \"options\": options}])\n        if not isinstance(baseline_pred, int):\n            return 0\n        num_tokens = input_ids.size(1)\n        sample_size = min(20, num_tokens)\n        sampled_idxs = torch.randperm(num_tokens)[:sample_size]\n        changes = 0\n        for idx in sampled_idxs:\n            ablated = input_ids.clone()\n            ablated[0, idx] = tokenizer.unk_token_id\n            ablated_text = tokenizer.decode(ablated[0], skip_special_tokens=True)\n            pred_ablated, _ = predict(model, [{\"prompt\": ablated_text, \"options\": options}])\n            if pred_ablated[0] != baseline_pred:\n                changes += 1\n        return changes / sample_size\n    except:\n        return 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_probing_accuracy(examples):\n    embeddings = []\n    labels = []\n\n    for ex in examples:\n        prompt = get_prompt(ex)\n        enc = prepare_input(prompt)\n        with torch.no_grad():\n            emb = model.transformer.wte(enc['input_ids']).mean(dim=1).cpu().numpy()\n        embeddings.append(emb[0])\n\n        # Try to get label from example\n        label = ex.get('label') or ex.get('gold') or ex.get('correct') or ex.get('answer')\n        if isinstance(label, str):\n            label = ord(label.upper()) - ord('A')\n        labels.append(label)\n\n    if len(set(labels)) < 2:\n        return None\n\n    clf = LogisticRegression(max_iter=1000)\n    clf.fit(embeddings, labels)\n    probing_accuracy = clf.score(embeddings, labels)\n\n    return probing_accuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_bertscore(references, candidates):\n    P, R, F1 = bert_score(candidates, references, lang=\"en\", verbose=False)\n    return list(F1.numpy())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def contrastive_distractor_swap(example):\n    prompt = get_prompt(example)\n    options = get_options(example)\n    if not options:\n        return None, None\n    swapped = options[::-1]\n    pred_orig = predict(model, [{\"prompt\": prompt, \"options\": options}])[0][0]\n    pred_swapped = predict(model, [{\"prompt\": prompt, \"options\": swapped}])[0][0]\n    return pred_orig, pred_swapped","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_dataset(dataset):\n    examples = load_data(data_dir + dataset[\"path\"])\n    rows = []\n    probing_acc = compute_probing_accuracy(examples)\n\n    for idx, example in enumerate(tqdm(examples)):\n        prompt = get_prompt(example)\n        options = get_options(example)\n        if not options:\n            continue\n        \n        preds, logits = predict(model, [{\"prompt\": prompt, \"options\": options}])\n        pred_idx = preds[0]\n        gold_idx = ord(example.get(\"answer\", \"A\").upper()) - ord('A')\n        \n        GLOBAL_Y_TRUE.append(gold_idx)\n        GLOBAL_Y_PRED.append(pred_idx)\n        GLOBAL_LOGITS.append(logits[0])\n\n        try:\n            heuristics = shallow_heuristic_check(example.get('question', ''), options)\n            heuristic_score = sum(heuristics) / len(heuristics) if heuristics else None\n        except:\n            heuristic_score = None\n        \n        try:\n            shap_val = fast_compute_shap_values(example)\n        except:\n            shap_val = None\n        \n        try:\n            contrastive_orig, contrastive_swap = contrastive_distractor_swap(example)\n            contrastive_orig = chr(65 + contrastive_orig) if contrastive_orig is not None else None\n            contrastive_swap = chr(65 + contrastive_swap) if contrastive_swap is not None else None\n            contrastive_robust = contrastive_orig == contrastive_swap\n        except:\n            contrastive_orig, contrastive_swap, contrastive_robust = None, None, None\n\n        row = {\n            \"id\": idx,\n            \"dataset\": dataset[\"name\"],\n            \"question\": example.get(\"question\", \"\"),\n            \"option_0\": options[0] if len(options) > 0 else \"\",\n            \"option_1\": options[1] if len(options) > 1 else \"\",\n            \"option_2\": options[2] if len(options) > 2 else \"\",\n            \"option_3\": options[3] if len(options) > 3 else \"\",\n            \"probing_accuracy\": probing_acc,\n            \"logits\": logits[0],\n            \"prediction\": pred_idx,\n            \"actual\": gold_idx,\n            \"bertscore_f1\": run_bertscore([options[gold_idx]], [options[pred_idx]])[0],\n            \"heuristic_dependence_score\": heuristic_score,\n            \"reasoning_score\": reasoning_score(model, prompt, options),\n            \"context_fidelity_score\": context_fidelity_score(model, example),\n            \"shap_score\": shap_val,\n            \"contrastive_prediction_original\": contrastive_orig,\n            \"contrastive_prediction_swapped\": contrastive_swap,\n            \"contrastive_robust\": contrastive_robust\n        }\n\n        rows.append(row)\n\n    results_df = pd.DataFrame(rows)\n    results_df.to_csv(os.path.join(save_dir, f\"{dataset['name']}_results.csv\"), index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_reasoning_context(save_dir, dataset_name):\n    results_path = os.path.join(save_dir, f\"{dataset_name}_results.csv\")\n    df = pd.read_csv(results_path)\n\n    reasoning = df['reasoning_score'].dropna()\n    context_fidelity = df['context_fidelity_score'].dropna()\n    # heuristic_dependence = df['heuristic_dependence_score'].dropna()  \n    fig, ax = plt.subplots(2, 1, figsize=(8, 10))  \n    sns.histplot(reasoning, ax=ax[0], kde=False, color=\"steelblue\", edgecolor='black', linewidth=0.6)\n    ax[0].set_title(\"Reasoning Score\", fontsize=18, fontweight='bold')\n    ax[0].set_xlabel(\"Score\", fontsize=14)\n    ax[0].set_ylabel(\"Frequency\", fontsize=14)\n    ax[0].tick_params(axis='both', labelsize=12)\n    ax[0].grid(False)\n\n    sns.histplot(context_fidelity, ax=ax[1], kde=False, color=\"steelblue\", edgecolor='black', linewidth=0.6)\n    ax[1].set_title(\"Context Fidelity\", fontsize=18, fontweight='bold')\n    ax[1].set_xlabel(\"Score\", fontsize=14)\n    ax[1].set_ylabel(\"Frequency\", fontsize=14)\n    ax[1].tick_params(axis='both', labelsize=12)\n    ax[1].grid(False)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_softmax_distribution():\n    all_softmax = [np.exp(logit) / np.sum(np.exp(logit)) for logit in GLOBAL_LOGITS]\n    all_softmax_flat = np.concatenate(all_softmax)\n    plt.figure(figsize=(6, 5))\n    sns.histplot(all_softmax_flat, bins=50, color='steelblue')\n    plt.title(\"Softmax Probability Distribution\", fontsize=18)\n    plt.xlabel(\"Softmax Probability\", fontsize=14)\n    plt.ylabel(\"Frequency\", fontsize=14)\n    plt.grid(False)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def final_confusion_matrix():\n    acc = accuracy_score(GLOBAL_Y_TRUE, GLOBAL_Y_PRED)\n    cm = confusion_matrix(GLOBAL_Y_TRUE, GLOBAL_Y_PRED)\n    print(f\"Accuracy: {acc:.3f}\")\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def datamap_confidence_scatter():\n    correct = np.array([int(p == y) for p, y in zip(GLOBAL_Y_PRED, GLOBAL_Y_TRUE)])\n    conf = np.max(np.array(GLOBAL_LOGITS), axis=1)\n    plt.figure(figsize=(8, 6))\n    sns.scatterplot(x=conf, y=correct, hue=correct, palette=\"coolwarm\")\n    plt.xlabel(\"Max Confidence\")\n    plt.ylabel(\"Correct Prediction\")\n    plt.title(\"DataMap Scatter: Confidence vs Accuracy\")\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example = load_data(data_dir + datasets[0][\"path\"])[0]\nprompt = get_prompt(example)\nattn_map, tokens = get_attention_map(model, prompt)\nplot_attention_heatmap(attn_map, tokens)\nrollout_map, rollout_tokens = true_attention_rollout(model, prompt)\nplot_attention_rollout(rollout_map, rollout_tokens)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for ds in datasets:\n    process_dataset(ds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_reasoning_context(save_dir, dataset_name=\"SATACT\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_softmax_distribution()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_confusion_matrix()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datamap_confidence_scatter()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# random example\nexample = random.choice(load_data(data_dir + datasets[0][\"path\"]))\nprompt = get_prompt(example)\nconfidences = logit_lens_visual(model, prompt)\n\nplt.figure(figsize=(8, 6))\nplt.plot(confidences, marker='o')\nplt.title(\"Logit Lens: Token-wise Confidence (Random Example)\", fontsize=18, fontweight='bold')\nplt.xlabel(\"Token Position\", fontsize=14)\nplt.ylabel(\"Max Softmax Confidence\", fontsize=14)\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}